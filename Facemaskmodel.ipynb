{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "082f0619-d9fc-4acd-8054-d7001d4fc397",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense,Dropout,Flatten,Activation\n",
    "from tensorflow.keras.preprocessing.image import img_to_array,load_img,ImageDataGenerator\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.applications import MobileNetV2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "58e5d878-bf9b-4485-95d5-f7ec62fc1496",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting mask folders with mask and without mask from current directory\n",
    "\n",
    "mask_folder=\"./with_mask\"\n",
    "no_mask_folder=\"./without_mask\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "68b56e9e-6135-46cd-8f56-429c13075a72",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_data=[]\n",
    "labels=[]\n",
    "def load_imgdata(mask_folder,no_mask_folder):\n",
    "    for folder,label in [(mask_folder,1),(no_mask_folder,0)]:\n",
    "        for img_name in os.listdir(folder):\n",
    "            img_path=os.path.join(folder,img_name)\n",
    "            image=load_img(img_path,target_size=(100,100))\n",
    "            image_array=img_to_array(image)\n",
    "            img_data.append(image_array)\n",
    "            labels.append(label)\n",
    "    return np.array(img_data),np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3884bfc6-c983-4d51-98b2-9bdebbaf7a87",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\Lib\\site-packages\\PIL\\Image.py:1000: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "img_data,label=load_imgdata(mask_folder,no_mask_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "63d1662b-020d-42c9-a3d8-d532245ecaa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_data=img_data/255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f5382351-1798-4b36-8d22-3b3b13711a00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Samples : 2692 Testing Samples : 1154\n"
     ]
    }
   ],
   "source": [
    "x_train,x_test,y_train,y_test=train_test_split(img_data,label,test_size=0.3,random_state=42)\n",
    "print(f\"Training Samples : {len(x_train)} Testing Samples : {len(x_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "dfe48910-b451-4d2a-a5a6-296e0021108b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_15172\\1471432744.py:1: UserWarning: `input_shape` is undefined or non-square, or `rows` is not in [96, 128, 160, 192, 224]. Weights for input shape (224, 224) will be loaded as the default.\n",
      "  base_model=MobileNetV2(input_shape=(100,100,3),include_top=False,weights=\"imagenet\")\n"
     ]
    }
   ],
   "source": [
    "base_model=MobileNetV2(input_shape=(100,100,3),include_top=False,weights=\"imagenet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "79b15ae1-523f-4ec4-a513-42ce7dc0ae68",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model.trainable=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b98ff1c5-896c-4fd8-9af2-c0ec088df109",
   "metadata": {},
   "outputs": [],
   "source": [
    "model=Sequential([\n",
    "                  base_model,\n",
    "                  Flatten(),\n",
    "                  Dense(128,activation='relu'),\n",
    "                  Dropout(0.3),\n",
    "                  Dense(1,activation='sigmoid')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2e4d8137-eca8-4f97-a43f-dc919e95e10e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',loss='binary_crossentropy',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ec8bc1dd-932e-425a-a0b0-e932e7b55dde",
   "metadata": {},
   "outputs": [],
   "source": [
    "datagen=ImageDataGenerator(\n",
    "  rotation_range=20,\n",
    "  width_shift_range=0.2,\n",
    "  height_shift_range=0.2,\n",
    "  shear_range=0.2,\n",
    "  zoom_range=0.2,\n",
    "  horizontal_flip=True,\n",
    "  fill_mode='nearest'\n",
    " )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "afe947a9-0690-4638-8027-6a26497cae10",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\Lib\\site-packages\\keras\\src\\trainers\\data_adapters\\py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m85/85\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 257ms/step - accuracy: 0.8425 - loss: 1.8785 - val_accuracy: 0.9827 - val_loss: 0.0648\n",
      "Epoch 2/10\n",
      "\u001b[1m85/85\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 236ms/step - accuracy: 0.9704 - loss: 0.1032 - val_accuracy: 0.9870 - val_loss: 0.0549\n",
      "Epoch 3/10\n",
      "\u001b[1m85/85\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 232ms/step - accuracy: 0.9720 - loss: 0.0721 - val_accuracy: 0.9818 - val_loss: 0.0504\n",
      "Epoch 4/10\n",
      "\u001b[1m85/85\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 235ms/step - accuracy: 0.9658 - loss: 0.1107 - val_accuracy: 0.9913 - val_loss: 0.0325\n",
      "Epoch 5/10\n",
      "\u001b[1m85/85\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 235ms/step - accuracy: 0.9781 - loss: 0.0674 - val_accuracy: 0.9931 - val_loss: 0.0311\n",
      "Epoch 6/10\n",
      "\u001b[1m85/85\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 232ms/step - accuracy: 0.9660 - loss: 0.0949 - val_accuracy: 0.9818 - val_loss: 0.0660\n",
      "Epoch 7/10\n",
      "\u001b[1m85/85\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 232ms/step - accuracy: 0.9756 - loss: 0.0594 - val_accuracy: 0.9896 - val_loss: 0.0387\n",
      "Epoch 8/10\n",
      "\u001b[1m85/85\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 236ms/step - accuracy: 0.9688 - loss: 0.0872 - val_accuracy: 0.9913 - val_loss: 0.0379\n",
      "Epoch 9/10\n",
      "\u001b[1m85/85\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 239ms/step - accuracy: 0.9708 - loss: 0.0723 - val_accuracy: 0.9896 - val_loss: 0.0460\n",
      "Epoch 10/10\n",
      "\u001b[1m85/85\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 241ms/step - accuracy: 0.9719 - loss: 0.0844 - val_accuracy: 0.9913 - val_loss: 0.0353\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x153a683e8d0>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(datagen.flow(x_train,y_train,batch_size=32),epochs=10,validation_data=(x_test,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "cf0a8130-38bc-405d-b882-43f195595df6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 105ms/step - accuracy: 0.9905 - loss: 0.0548\n",
      "Accuracy : 99.13%\n"
     ]
    }
   ],
   "source": [
    "loss,accuracy=model.evaluate(x_test,y_test)\n",
    "print(f\"Accuracy : {accuracy*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d3f41ede-b413-42cc-81e1-802b84745fd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    }
   ],
   "source": [
    "model.save(\"mask.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a9f5d23-e9ba-4072-b415-6dc9270a6f51",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
