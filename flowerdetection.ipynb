{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d8307b6b-068b-464c-8c08-3f75d8f8e0f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.models import Sequential\n",
    "from keras.layers import Dense,Conv2D,MaxPooling2D,Dropout,Flatten\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.layers import RandomZoom, RandomContrast, RandomRotation\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "import cv2\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "69a13e6c-4081-4782-8743-cdb2af9c7191",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.18.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cf05d2b8-f360-4588-850a-d0eef63108f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "datagen = ImageDataGenerator(\n",
    "    rotation_range=20,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='nearest',\n",
    "    preprocessing_function=tf.keras.applications.mobilenet_v2.preprocess_input \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8ecef07e-9dae-4e40-8436-a6c0559fffa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "flowers_directory=Path('flowers')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "31457b4f-6224-472d-a02c-1a061db1361e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WindowsPath('flowers')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flowers_directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8ce27cea-14fc-441b-8f4a-279dcc5df6d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[WindowsPath('flowers/daisy'),\n",
       " WindowsPath('flowers/dandelion'),\n",
       " WindowsPath('flowers/rose'),\n",
       " WindowsPath('flowers/sunflower'),\n",
       " WindowsPath('flowers/tulip')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(flowers_directory.glob('*'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e65d2ca6-b3f7-4cab-8dc0-5c78c511a3e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "784"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list(flowers_directory.glob('rose/*')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bb113a6a-a293-4810-b714-5849bc74ac4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "flower_img_dict={\n",
    "                 'daisy':list(flowers_directory.glob('daisy/*')),\n",
    "                 'dandelion':list(flowers_directory.glob('dandelion/*')),\n",
    "                 'rose':list(flowers_directory.glob('rose/*')),\n",
    "                 'sunflower':list(flowers_directory.glob('sunflower/*')),\n",
    "                 'tulip':list(flowers_directory.glob('tulip/*'))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d857a0fd-b2c3-4a27-a0d3-5ffe0944d050",
   "metadata": {},
   "outputs": [],
   "source": [
    "flower_img_labels={\n",
    "                 'daisy':0,\n",
    "                 'dandelion':1,\n",
    "                 'rose':2,\n",
    "                 'sunflower':3,\n",
    "                 'tulip':4  \n",
    "}\n",
    "                    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09d3ba33-0f3d-41f9-950c-6476da53c826",
   "metadata": {},
   "source": [
    "flower_name refers to the key of the dictionary, which represents the name of the flower (like 'daisy', 'dandelion', etc.).\n",
    "\n",
    "image refers to the value associated with that key, which is a list of image paths for that particular flower."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "815808a6-f0dc-4eb5-94ad-d50928948981",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "daisy\n",
      "765\n",
      "dandelion\n",
      "1052\n",
      "rose\n",
      "784\n",
      "sunflower\n",
      "733\n",
      "tulip\n",
      "984\n"
     ]
    }
   ],
   "source": [
    "x=[]\n",
    "y=[]\n",
    "accepted_extensions = ['.jpg', '.jpeg', '.png', '.bmp', '.gif']\n",
    "for flower_name,image in flower_img_dict.items():\n",
    "    print(flower_name)\n",
    "    print(len(image))\n",
    "    for img in image:\n",
    "        # Skip non-image files (e.g., folders or checkpoint files)\n",
    "        if '.ipynb_checkpoints' in str(img):\n",
    "            continue\n",
    "        if not img.suffix.lower() in accepted_extensions:\n",
    "            continue  # Skip this file and move to the next one\n",
    "        flower_img=cv2.imread(str(img))\n",
    "        # Check if the image was loaded successfully\n",
    "        if flower_img is None:\n",
    "            continue  # Skip this image and continue to the next one\n",
    "        resize_img=cv2.resize(flower_img,(224,224))\n",
    "        x.append(resize_img)\n",
    "        y.append(flower_img_labels[flower_name])      \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fc13413d-5c89-4dc4-97d9-a63286301410",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 0, 0, 0, 0]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f89c6e3d-64ef-4fce-90a5-3217c1da64bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.2,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fc98ed70-3a85-428f-bf89-00e5eb8331f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = np.array(x_train, dtype='int32')\n",
    "x_test = np.array(x_test, dtype='int32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f1a17a33-39c6-4871-90fb-e001a809cc40",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train=x_train/255\n",
    "x_test=x_test/255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ffadbb31-cdd9-49d1-b0f4-ec3a6e877708",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_generator = datagen.flow(x_train, y_train, batch_size=32, shuffle=True, seed=42)\n",
    "test_generator = datagen.flow(x_test, y_test, batch_size=32, shuffle=False, seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b122198d-d54b-4d96-a43d-ada1d5d4367f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications.mobilenet_v2 import MobileNetV2, preprocess_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "60b019ee-cd7a-4875-b3c6-568e63314bc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load MobileNetV2 without the top classification layer\n",
    "base_model = MobileNetV2(input_shape=(224, 224, 3), include_top=False, weights='imagenet')\n",
    "\n",
    "# Unfreeze the last few layers of the base model\n",
    "base_model.trainable = True\n",
    "for layer in base_model.layers[:-10]:  # Unfreeze the last 10 layers\n",
    "    layer.trainable = False\n",
    "\n",
    "\n",
    "# Build the model\n",
    "model = Sequential([\n",
    "    base_model,\n",
    "    MaxPooling2D(),\n",
    "    Flatten(),\n",
    "    Dense(1024, activation='relu'),\n",
    "    Dropout(0.2),\n",
    "    Dense(5, activation='softmax')  # Adjust number of classes\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "16ef377e-5a9b-44e7-b274-21f12f8c4a64",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\Lib\\site-packages\\keras\\src\\trainers\\data_adapters\\py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m107/107\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m172s\u001b[0m 2s/step - accuracy: 0.2266 - loss: 11.8658 - val_accuracy: 0.2188 - val_loss: 135.8250\n",
      "Epoch 2/10\n",
      "\u001b[1m  1/107\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:30\u001b[0m 857ms/step - accuracy: 0.2188 - loss: 1.6506"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\Lib\\site-packages\\keras\\src\\trainers\\epoch_iterator.py:107: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
      "  self._interrupted_warning()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m107/107\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 244ms/step - accuracy: 0.2188 - loss: 1.6506 - val_accuracy: 0.2188 - val_loss: 135.4723\n",
      "Epoch 3/10\n",
      "\u001b[1m107/107\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m143s\u001b[0m 1s/step - accuracy: 0.2672 - loss: 1.5979 - val_accuracy: 0.2188 - val_loss: 126.8079\n",
      "Epoch 4/10\n",
      "\u001b[1m107/107\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 230ms/step - accuracy: 0.3750 - loss: 1.5479 - val_accuracy: 0.2188 - val_loss: 126.8273\n",
      "Epoch 5/10\n",
      "\u001b[1m107/107\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m139s\u001b[0m 1s/step - accuracy: 0.2992 - loss: 1.5728 - val_accuracy: 0.2188 - val_loss: 109.0525\n",
      "Epoch 6/10\n",
      "\u001b[1m107/107\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 253ms/step - accuracy: 0.1875 - loss: 1.6046 - val_accuracy: 0.2188 - val_loss: 108.7263\n",
      "Epoch 7/10\n",
      "\u001b[1m107/107\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m153s\u001b[0m 1s/step - accuracy: 0.2924 - loss: 1.5663 - val_accuracy: 0.2188 - val_loss: 85.4883\n",
      "Epoch 8/10\n",
      "\u001b[1m107/107\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 248ms/step - accuracy: 0.3438 - loss: 1.5404 - val_accuracy: 0.2188 - val_loss: 85.3439\n",
      "Epoch 9/10\n",
      "\u001b[1m107/107\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m143s\u001b[0m 1s/step - accuracy: 0.3116 - loss: 1.5476 - val_accuracy: 0.2188 - val_loss: 60.4258\n",
      "Epoch 10/10\n",
      "\u001b[1m107/107\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 221ms/step - accuracy: 0.2812 - loss: 1.5286 - val_accuracy: 0.2188 - val_loss: 60.1094\n"
     ]
    }
   ],
   "source": [
    "#7. Train the Model Using the Data Generators:\n",
    "\n",
    "#Train the model using your training and testing data generators:​\n",
    "\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    epochs=10,\n",
    "    validation_data=test_generator,\n",
    "    steps_per_epoch=len(x_train) // 32,\n",
    "    validation_steps=len(x_test) // 32\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0eb949b-193d-402c-aa9e-58e40abfdf00",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
